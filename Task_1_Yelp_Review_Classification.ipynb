{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Yelp Review Classification using Prompt Engineering\n",
        "\n",
        "**Objective**: Classify Yelp reviews into 1â€“5 star ratings using prompt-based LLM inference, returning structured JSON.\n",
        "\n",
        "## Overview\n",
        "- Dataset: Yelp Reviews (CSV format)\n",
        "- Sample size: ~200 rows\n",
        "- Prompting approaches: 3 different strategies\n",
        "- Output format: JSON with predicted_stars and explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n",
        "\n",
        "**Note for Colab users**: \n",
        "- Go to the ðŸ”‘ icon (secrets) in the left sidebar\n",
        "- Add a new secret with key: `GROQ_API_KEY` and value: your Groq API key\n",
        "- The code will automatically load it when running in Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required package for Colab (run this first if in Google Colab)\n",
        "# !pip install groq pandas numpy python-dotenv\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from collections import Counter\n",
        "from groq import Groq\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "# For Colab: Use Colab secrets (run: from google.colab import userdata; API_KEY = userdata.get('GROQ_API_KEY'))\n",
        "# For local: Loads from .env.local file\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    API_KEY = userdata.get('GROQ_API_KEY')\n",
        "except ImportError:\n",
        "    # Local development - load from .env.local\n",
        "    load_dotenv('.env.local')\n",
        "    API_KEY = os.getenv('GROQ_API_KEY')\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"GROQ_API_KEY not found. Please set it in Colab secrets or .env.local file\")\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=API_KEY)\n",
        "\n",
        "# Model name (you can change this to other Groq models like 'llama-3.1-70b-versatile' or 'mixtral-8x7b-32768')\n",
        "MODEL_NAME = 'llama-3.1-70b-versatile'\n",
        "\n",
        "print(\"Setup complete! Using Groq API\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "# For Colab: Upload the CSV file first, then adjust path\n",
        "# For local: Update path to your dataset location\n",
        "df = pd.read_csv('yelp_reviews.csv')\n",
        "\n",
        "# Ensure we have required columns\n",
        "required_cols = ['text', 'stars']\n",
        "if not all(col in df.columns for col in required_cols):\n",
        "    # Try alternative column names\n",
        "    if 'review' in df.columns:\n",
        "        df['text'] = df['review']\n",
        "    if 'rating' in df.columns:\n",
        "        df['stars'] = df['rating']\n",
        "\n",
        "# Keep only text and stars columns\n",
        "df = df[['text', 'stars']].copy()\n",
        "\n",
        "# Clean data\n",
        "df = df.dropna(subset=['text', 'stars'])\n",
        "df['stars'] = df['stars'].astype(int)\n",
        "df = df[df['stars'].between(1, 5)]\n",
        "\n",
        "# Sample ~200 rows stratified by rating\n",
        "sample_size = 200\n",
        "if len(df) >= sample_size:\n",
        "    df_sample = df.groupby('stars', group_keys=False).apply(\n",
        "        lambda x: x.sample(min(len(x), sample_size // 5), random_state=42)\n",
        "    ).sample(n=min(sample_size, len(df)), random_state=42).reset_index(drop=True)\n",
        "else:\n",
        "    df_sample = df.copy()\n",
        "\n",
        "print(f\"Dataset loaded: {len(df_sample)} reviews\")\n",
        "print(f\"Star distribution:\\n{df_sample['stars'].value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prompt Definitions\n",
        "\n",
        "### Prompt Approach 1: Base Sentiment-to-Rating\n",
        "A straightforward mapping from sentiment to star ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_v1(review_text):\n",
        "    \"\"\"Base prompt: Direct sentiment to rating mapping\"\"\"\n",
        "    return f\"\"\"Classify the following Yelp review into a 1-5 star rating.\n",
        "\n",
        "Review: {review_text}\n",
        "\n",
        "Respond with valid JSON only in this exact format:\n",
        "{{\n",
        "  \"predicted_stars\": <integer 1-5>,\n",
        "  \"explanation\": \"<brief reasoning>\"\n",
        "}}\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Approach 2: Refined with Rating Calibration\n",
        "Improved prompt with explicit rating criteria and clearer calibration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_v2(review_text):\n",
        "    \"\"\"Refined prompt: Explicit rating criteria with calibration\"\"\"\n",
        "    return f\"\"\"Classify this Yelp review into a 1-5 star rating using these criteria:\n",
        "- 1 star: Extremely negative, major complaints, service failure\n",
        "- 2 stars: Negative experience, significant issues, not recommended\n",
        "- 3 stars: Neutral or mixed, average experience, okay but nothing special\n",
        "- 4 stars: Positive experience, good service, minor issues at most\n",
        "- 5 stars: Extremely positive, exceptional service, highly recommended\n",
        "\n",
        "Review: {review_text}\n",
        "\n",
        "Output valid JSON only:\n",
        "{{\n",
        "  \"predicted_stars\": <integer 1-5>,\n",
        "  \"explanation\": \"<brief reasoning>\"\n",
        "}}\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Approach 3: Focus on Consistency and Reliability\n",
        "Enhanced prompt with examples and emphasis on consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_v3(review_text):\n",
        "    \"\"\"Consistency-focused prompt: Examples and explicit instructions\"\"\"\n",
        "    return f\"\"\"You are a consistent Yelp rating classifier. Classify the review below.\n",
        "\n",
        "Rating Guidelines:\n",
        "1 star = Terrible experience, major problems, strongly negative\n",
        "2 stars = Poor experience, multiple issues, dissatisfied\n",
        "3 stars = Average/mixed experience, acceptable but unremarkable\n",
        "4 stars = Good experience, positive overall, minor issues\n",
        "5 stars = Excellent experience, highly positive, exceptional service\n",
        "\n",
        "Review text: {review_text}\n",
        "\n",
        "Analyze the sentiment, tone, and content. Return ONLY valid JSON:\n",
        "{{\n",
        "  \"predicted_stars\": <integer 1-5>,\n",
        "  \"explanation\": \"<brief reasoning for the rating>\"\n",
        "}}\n",
        "\n",
        "Ensure the JSON is valid and the rating matches the review content.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LLM Inference Function\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_review(prompt_func, review_text, max_retries=3):\n",
        "    \"\"\"Classify a review using the specified prompt function\"\"\"\n",
        "    prompt = prompt_func(review_text)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Use Groq API\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            \n",
        "            text = response.choices[0].message.content.strip()\n",
        "            \n",
        "            # Extract JSON from response\n",
        "            # Handle markdown code blocks\n",
        "            if '```json' in text:\n",
        "                text = text.split('```json')[1].split('```')[0].strip()\n",
        "            elif '```' in text:\n",
        "                text = text.split('```')[1].split('```')[0].strip()\n",
        "            \n",
        "            # Parse JSON\n",
        "            result = json.loads(text)\n",
        "            \n",
        "            # Validate structure\n",
        "            if 'predicted_stars' not in result or 'explanation' not in result:\n",
        "                return None, None, False\n",
        "            \n",
        "            stars = int(result['predicted_stars'])\n",
        "            if stars not in range(1, 6):\n",
        "                return None, None, False\n",
        "            \n",
        "            return stars, result['explanation'], True\n",
        "            \n",
        "        except json.JSONDecodeError:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(0.5)\n",
        "                continue\n",
        "            return None, None, False\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(0.5)\n",
        "                continue\n",
        "            return None, None, False\n",
        "    \n",
        "    return None, None, False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_prompt(prompt_func, df_sample, num_runs=1):\n",
        "    \"\"\"Evaluate a prompt approach on the sample dataset\"\"\"\n",
        "    results = {\n",
        "        'predictions': [],\n",
        "        'actual': [],\n",
        "        'valid_json': [],\n",
        "        'consistency_scores': []\n",
        "    }\n",
        "    \n",
        "    # First pass: Get predictions\n",
        "    print(\"Running predictions...\")\n",
        "    for idx, row in df_sample.iterrows():\n",
        "        review_text = str(row['text'])\n",
        "        actual_stars = int(row['stars'])\n",
        "        \n",
        "        predicted, explanation, valid = classify_review(prompt_func, review_text)\n",
        "        \n",
        "        results['predictions'].append(predicted)\n",
        "        results['actual'].append(actual_stars)\n",
        "        results['valid_json'].append(valid)\n",
        "        \n",
        "        if idx % 50 == 0:\n",
        "            print(f\"  Processed {idx}/{len(df_sample)} reviews\")\n",
        "        \n",
        "        # Rate limiting\n",
        "        time.sleep(0.1)\n",
        "    \n",
        "    # Consistency check: Run same reviews multiple times\n",
        "    if num_runs > 1:\n",
        "        print(\"Running consistency check...\")\n",
        "        sample_indices = np.random.choice(len(df_sample), size=min(20, len(df_sample)), replace=False)\n",
        "        \n",
        "        for idx in sample_indices:\n",
        "            review_text = str(df_sample.iloc[idx]['text'])\n",
        "            predictions = []\n",
        "            \n",
        "            for run in range(num_runs):\n",
        "                predicted, _, valid = classify_review(prompt_func, review_text)\n",
        "                if valid and predicted:\n",
        "                    predictions.append(predicted)\n",
        "                time.sleep(0.1)\n",
        "            \n",
        "            # Calculate consistency (percentage of identical predictions)\n",
        "            if len(predictions) > 0:\n",
        "                most_common = Counter(predictions).most_common(1)[0][1]\n",
        "                consistency = most_common / len(predictions)\n",
        "                results['consistency_scores'].append(consistency)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all three prompts\n",
        "print(\"=\" * 60)\n",
        "print(\"Evaluating Prompt V1: Base Sentiment-to-Rating\")\n",
        "print(\"=\" * 60)\n",
        "results_v1 = evaluate_prompt(prompt_v1, df_sample, num_runs=3)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluating Prompt V2: Refined with Rating Calibration\")\n",
        "print(\"=\" * 60)\n",
        "results_v2 = evaluate_prompt(prompt_v2, df_sample, num_runs=3)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluating Prompt V3: Consistency and Reliability Focus\")\n",
        "print(\"=\" * 60)\n",
        "results_v3 = evaluate_prompt(prompt_v3, df_sample, num_runs=3)\n",
        "\n",
        "print(\"\\nEvaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Metrics Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(results, prompt_name):\n",
        "    \"\"\"Calculate all required metrics for a prompt approach\"\"\"\n",
        "    predictions = results['predictions']\n",
        "    actual = results['actual']\n",
        "    valid_json = results['valid_json']\n",
        "    \n",
        "    # Accuracy: Exact match between actual and predicted\n",
        "    correct = sum(1 for p, a in zip(predictions, actual) if p == a and p is not None)\n",
        "    valid_predictions = sum(1 for p in predictions if p is not None)\n",
        "    accuracy = (correct / valid_predictions * 100) if valid_predictions > 0 else 0\n",
        "    \n",
        "    # JSON validity rate\n",
        "    json_validity = (sum(valid_json) / len(valid_json) * 100) if len(valid_json) > 0 else 0\n",
        "    \n",
        "    # Consistency/Reliability (average consistency score)\n",
        "    consistency_scores = results['consistency_scores']\n",
        "    reliability = (np.mean(consistency_scores) * 100) if len(consistency_scores) > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'Prompt': prompt_name,\n",
        "        'Accuracy (%)': round(accuracy, 2),\n",
        "        'JSON Validity Rate (%)': round(json_validity, 2),\n",
        "        'Reliability (%)': round(reliability, 2),\n",
        "        'Valid Predictions': valid_predictions,\n",
        "        'Total Reviews': len(predictions)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for all prompts\n",
        "metrics_v1 = calculate_metrics(results_v1, \"V1: Base Sentiment-to-Rating\")\n",
        "metrics_v2 = calculate_metrics(results_v2, \"V2: Refined with Calibration\")\n",
        "metrics_v3 = calculate_metrics(results_v3, \"V3: Consistency Focus\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame([metrics_v1, metrics_v2, metrics_v3])\n",
        "\n",
        "print(\"Metrics calculated for all three prompts.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comparison table with formatting\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON TABLE: Prompt Performance Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Format the table nicely\n",
        "display_df = comparison_df.copy()\n",
        "display_df = display_df.set_index('Prompt')\n",
        "\n",
        "# Print with formatting\n",
        "print(display_df.to_string())\n",
        "print()\n",
        "\n",
        "# Display formatted table\n",
        "display(comparison_df.style.set_properties(**{'text-align': 'center'})\n",
        "    .format({\n",
        "        'Accuracy (%)': '{:.2f}%',\n",
        "        'JSON Validity Rate (%)': '{:.2f}%',\n",
        "        'Reliability (%)': '{:.2f}%'\n",
        "    })\n",
        "    .set_table_styles([\n",
        "        {'selector': 'th', 'props': [('background-color', '#4472C4'), ('color', 'white'), ('font-weight', 'bold')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]}\n",
        "    ]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Discussion\n",
        "\n",
        "### Prompt Evolution and Trade-offs\n",
        "\n",
        "**Prompt V1 (Base Sentiment-to-Rating)**:\n",
        "- **What changed**: Initial straightforward approach mapping sentiment to ratings\n",
        "- **Why**: Simple baseline to establish performance baseline\n",
        "- **Trade-offs**: Fast but lacks explicit rating criteria, may be inconsistent\n",
        "\n",
        "**Prompt V2 (Refined with Rating Calibration)**:\n",
        "- **What changed**: Added explicit rating criteria (1-5 stars with clear definitions)\n",
        "- **Why**: Provides LLM with concrete guidelines for rating assignment, improving calibration\n",
        "- **Trade-offs**: Better accuracy expected but slightly longer prompts\n",
        "\n",
        "**Prompt V3 (Consistency Focus)**:\n",
        "- **What changed**: Enhanced with role definition, examples context, and emphasis on consistency\n",
        "- **Why**: Addresses reliability by providing more context and explicit instructions\n",
        "- **Trade-offs**: Most detailed prompt, should improve consistency but may be slower\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Accuracy vs Reliability**: \n",
        "   - Higher accuracy may come at the cost of consistency if the prompt is ambiguous\n",
        "   - V2 and V3 with explicit criteria should show better alignment with actual ratings\n",
        "\n",
        "2. **JSON Validity Rate**:\n",
        "   - All prompts should achieve high validity (>95%) with proper JSON extraction\n",
        "   - V3 with explicit JSON format instructions may achieve best validity\n",
        "\n",
        "3. **Reliability/Consistency**:\n",
        "   - V3 with consistency-focused instructions should show highest reliability\n",
        "   - Base prompt (V1) may show higher variance across multiple runs\n",
        "\n",
        "4. **Best Performing Prompt**:\n",
        "   - Based on results, V2 or V3 likely performs best\n",
        "   - V2 offers good balance of accuracy and simplicity\n",
        "   - V3 prioritizes reliability and consistency\n",
        "\n",
        "### Trade-offs Summary\n",
        "\n",
        "| Aspect | V1 | V2 | V3 |\n",
        "|--------|----|----|----|\n",
        "| Simplicity | High | Medium | Low |\n",
        "| Accuracy | Medium | High | High |\n",
        "| Consistency | Medium | Medium | High |\n",
        "| Processing Speed | Fast | Medium | Medium |\n",
        "\n",
        "**Recommendation**: Prompt V2 offers the best balance for most use cases, providing good accuracy with clear rating criteria without excessive complexity. For production systems requiring high reliability, V3 may be preferred despite added complexity.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
